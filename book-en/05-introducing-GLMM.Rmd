# (PART\*) Generalized linear mixed models (GLMM) in `R` {-}

# Introduction to GLMM

Generalized linear mixed models (GLMM) are an extension of generalized linear models (GLM) that account for additional structure in dataset.

They follows similar steps to those we just introduced with linear mixed models (LMM):

- **1.** Incorporate random effects (like LMMs)

- **2.** Handle non-normal data, letting errors take on different distribution families - e.g. Poisson or negative binomial (like GLMs; Workshop 6)

---

As with the LMM portion of this workshop, we are going to work through the GLMM material with a dataset in order to better understand how GLMMs work and how to implement them in `R`.

In the Arabidopsis dataset, the effect of nutrient availability and herbivory (**fixed effects**) on the fruit production (**response variable**) of Arabidopsis thaliana was evaluated by measuring 625 plants across 9 different populations, each comprised of 2 to 3 different genotypes (**random effects**).

Start by importing the `Arabidopsis` dataset `banta_totalfruits.csv` into `R`.

```{r, echo = TRUE, eval = TRUE}
dat.tf <- read.csv("data/banta_totalfruits.csv")

# In this dataset, the column headers are defined as:
# popu factor with a level for each population
# gen factor with a level for each genotype
# nutrient factor with levels for low (value = 1) or high (value = 8)
# amd factor with levels for no damage or simulated herbivory
# total.fruits integer indicating the number of fruits per plant
```

# Choose an error distribution

Now we need to select an error distribution. This choice will be informed by the structure of our data. 

In the Arabidopsis dataset, the response variable is count data which suggests we need a **Poisson distribution** (i.e. the variance is equal to the mean).

Let's take a look:

```{r, echo = TRUE, eval = TRUE}
#Before we go any further, we need to select an error distribution. This choice will be informed by the structure of our data. 
# Our response variable is count data which suggests we need a Poisson distribution (i.e. the variance is equal to the mean).
hist(dat.tf$total.fruits, breaks = 50, col = 'blue', main = '',
     xlab = 'Total fruits', ylab = 'Count')
```

However, as we will soon see, the variance increases with the mean much more rapidly than expected under the Poisson distribution...

---

**Explore variance **

Let's take a closer look at the variance within our data.

To illustrate heterogeneity in variance we will first create boxplots of the **log** of total fruit production (**response variable**) versus different environmental factors.

Let's create new variables that represent every combination of **nutrient** x **clipping** x **random factor**

```{r, echo = TRUE, eval = TRUE}
# Let's explore the variance within our data
# Create new variables that represent every combination of variables
dat.tf <- within(dat.tf,
{
  # genotype x nutrient x clipping
  gna <- interaction(gen,nutrient,amd)
  gna <- reorder(gna, total.fruits, mean)
  # population x nutrient x clipping
  pna <- interaction(popu,nutrient,amd)
  pna <- reorder(pna, total.fruits, mean)
})
```

Now let's visualize:

```{r, echo = TRUE, eval = TRUE}
# Boxplot of total fruits vs genotype x nutrient x clipping interaction
ggplot(data = dat.tf, aes(factor(x = gna), y = log(total.fruits + 1))) +
  geom_boxplot(colour = "skyblue2", outlier.shape = 21,
  outlier.colour = "skyblue2") +
  ylab("log (Total fruits)\n") + # \n creates a space after the title
  xlab("\nGenotype x nutrient x clipping") + # space before the title
  theme_bw() + theme(axis.text.x = element_blank()) +
  stat_summary(fun = mean, geom = "point", colour = "red")

```


From this plot, we see that the variance of total fruits shows a large amount of heterogeneity among populations (population x nutrient x clipping interaction).

---

**Back to choosing an error distribution** 

As we just saw, there is a large amount of heterogeneity among group variances even when the response variable is transformed (i.e. log).

To determine which distribution family to use, we can run a diagnostic plot of the **group variances vs group means**. We provide an example below for the genotype x nutrient x clipping grouping.

- 1. If we observe a linear relationship between the variance and the mean with a slope = 1, then the Poisson family is appropriate,

- 2. If we observe a linear mean-variance relationship with a slope > 1 (i.e. Var = φµ where φ > 1), then the quasi-Poisson family (as introduced above) should be applied,

- 3. Finally, a quadratic relationship between the variance and the mean (i.e. $Var = µ(1 + α) or µ(1 + µ/k)$) is characteristic of overdispersed data that is driven by an underlying heterogeneity among samples. In this case, the negative binomial (Poisson-gamma) would be more appropriate.

```{r, echo = TRUE, eval = TRUE}
# Run a diagnostic lot of the group variances vs group means (genotype x nutrient x clipping grouping). 
# Code used to produce the plot : https://github.com/QCBSRworkshops/workshop07/blob/main/pres-fr/data/glmm_e.r
# Substantial variation among the sample variances on the transformed data
# For example, among genotypes:
grpVars <- tapply(dat.tf$total.fruits, dat.tf$gna, var)

grpMeans <- tapply(dat.tf$total.fruits,dat.tf$gna, mean)

# Quasi-Poisson
lm1 <- lm(grpVars~grpMeans-1) 
phi.fit <- coef(lm1)
# The -1 specifies a model with the intercept set to zero

# Negative binomial
lm2 <- lm(grpVars ~ I(grpMeans^2) + offset(grpMeans)-1)
k.fit <- 1/coef(lm2)
# The offset() is used to specify that we want the group means added as a term with its coefficient fixed to 1

# Non-parametric loess fit
Lfit <- loess(grpVars~grpMeans)

# The plot
plot(grpVars ~ grpMeans, xlab = "Group means", ylab = "Group variances" )
abline(a = 0, b = 1, lty = 2)
text(105,500, "Poisson")
curve(phi.fit*x, col = 2, add = TRUE)
# bquote() is used to substitute numeric values in equations with symbols
text(110,3900,
     bquote(paste("QP: ", sigma^2==.(round(phi.fit,1))*mu)), col = 2)
curve(x*(1+x/k.fit), col = 4, add = TRUE)
text(104,7200, paste("NB: k = ", round(k.fit, 1), sep = ""), col = 4)
mvec <- 0:120
lines(mvec, predict(Lfit, mvec), col = 5)
text(118, 2000, "loess", col = 5)
```

From the plot above we note that a linear quasi-Poisson may be better than the negative binomial, *but additional modeling is needed.*


# Poisson GLMM

**Given the mean-variance relationship, we will most likely need a model with over-dispersion.**

To understand why, let's start with a Poisson model.

To run a GLMM in `R` we will use the `glmer()` function from the `lme4` package:

```{r, echo = TRUE, eval = TRUE}
# Poisson GLMM
# Given the mean-variance relationship, we will most likely need a model with over-dispersion.
# To understand why, let's start with a Poisson model.
mp1 <- glmer(total.fruits ~ nutrient*amd + rack + status +
             (1|popu)+
             (1|gen),
             data = dat.tf, family = "poisson")
```

**Random effects**: `(1|popu)` and `(1|gen)`. We model random intercepts for both factors so that total fruit production can vary among populations (`popu`) and genotypes (`gen`).

---

**Over-dispersion check**

We can check for overdispersion using the `overdisp_fun()` function (Bolker *et al*. 2011) which divides the Pearson residuals by the residual degrees of freedom. 

The function tests whether **the ratio is greater than 1**.

Let's run the test:

```{r, echo = TRUE, eval = TRUE}
# Download the glmm_funs.R code from the wiki page and source it to run the function
source(file = "data/glmm_funs.R") # This line will vary depending on where your data is saved
# Over-dispersion check
overdisp_fun(mp1)
# Ratio is significantly > 1
```

**Ratio is significantly > 1**

As expected, we need to model a **different distribution** where the variance increases more rapidly than the mean.


# Negative binomial GLMM

One option for a distribution where the variance increases more rapidly with the mean is the **negative binomial** (or Poisson-gamma) distribution. Recall that the negative binomial distribution meets the assumption that the variance is proportional to the square of the mean.

We can model this distribution using the function `glmer.nb()`:

```{r, echo = TRUE, eval = TRUE}
# Negative binomial GLMM using the function glmer.nb()
mnb1 <- glmer.nb(total.fruits ~ nutrient*amd + rack + status +
                 (1|popu)+
                 (1|gen),
                 data = dat.tf,
                 control = glmerControl(optimizer = "bobyqa"))
# Control argument specifies the way we optimize the parameter values
```

We test again for over-dispersion:

```{r, echo = TRUE, eval = TRUE}
# Over-dispersion check
overdisp_fun(mnb1)
# Ratio is now much closer to 1 although p < 0.05
```

**Ratio is now much closer to 1 although p < 0.05**

# Poisson-lognormal GLMM

A second option for a distribution where the variance increases more rapidly with the mean is the **Poisson-lognormal** distribution.

This can be achieved simply by placing an **observation-level random effect** in the model formula.

See Harrison (2014) for further details https://doi.org/10.7717/peerj.616.

To do this in `R`, we first create a variable named `X`:

```{r, echo = TRUE, eval = TRUE}
# Poisson-lognormal GLMM

# This variable is already in your data "dat.tf", but here is how we create it :
dat.tf$X <- 1:nrow(dat.tf)
```

We take over-dispersion into account by adding the random effect `(1|X)` in the formula:

```{r, echo = TRUE, eval = TRUE}
# Account for over-dispersion
mpl1 <- glmer(total.fruits ~ nutrient*amd + rack + status +
              (1|X) +
              (1|popu)+
              (1|gen),
data = dat.tf, family = "poisson",
control = glmerControl(optimizer = "bobyqa"))
```

Finally, we test for over-dispersion:

```{r, echo = TRUE, eval = TRUE}
# Over-dispersion check
overdisp_fun(mpl1)
# Ratio now meets our criterion, thus, < 1
```

**Ratio is not <1 and meets our criterion!**

---

**Now that we've chosen out error distribution, let's visualize the model parameters.**

A graphical representation of the model parameters can be obtained using the `coefplot2()` function from the `coefplot2` package :

*Note: This package is not on CRAN! We install it from GitHub using the remotes package.*

```{R install_coefplot2}
if (!require("coefplot2"))
  remotes::install_github("palday/coefplot2", subdir = "pkg")
library(coefplot2)
```


```{r, echo = TRUE, eval = TRUE}
# Variance terms
coefplot2(mpl1, ptype = "vcov", intercept = TRUE, main = "Random effect variance")
```

```{r, echo = TRUE, eval = TRUE}
# Fixed effects
coefplot2(mpl1, intercept = TRUE, main = "Fixed effect coefficient")
```

*Note: error bars are only shown for the fixed effects because `glmer()` doesn't model uncertainty for random effects.*

---

**Now let's visualize the random effects**

You can extract the random effect predictions using `ranef()` and plot them using a `dotplot()` from the `lattice` package.

We observe differences **among population**: 

- Spanish populations (SP) have larger values than Swedish (SW) or Dutch (NL) populations

We observe mild differences **among genotypes**: 

- Difference among genotypes largely driven by genotype 34

```{r, echo = TRUE, eval = TRUE}
# dotplot code
pp <- list(layout.widths = list(left.padding = 0, right.padding = 0),
           layout.heights = list(top.padding = 0, bottom.padding = 0))
r2 <- ranef(mpl1, condVar = TRUE)
d2 <- dotplot(r2, par.settings = pp)

grid.arrange(d2$gen, d2$popu, nrow = 1)
```


