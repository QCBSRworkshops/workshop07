# (PART\*) Modèles Linéaires Généralisés Mixtes (GLMMs) en `R` {-}

# Introduction aux GLMM

Les modèles linéaires généralisés mixtes (GLMM) sont une extension des modèles linéaires généralisés (GLM) qui prennent en compte une structure supplémentaire dans l'ensemble des données.

Ils suivent des étapes similaires à celles que nous venons de présenter avec les modèles linéaires mixtes (LMM):

- **1.** Ils incorporent les effets aléatoires (comme les LMMs)
 
- **2.** Permettent de gérer des données non-normales (en laissant les erreurs prendre différentes familles de distribution - e.g Poisson ou binomial négatif) (comme les GLMs; atelier 6)


---

Comme pour la partie LMM de cet atelier, nous allons travailler sur le matériel GLMM avec un ensemble de données afin de mieux comprendre le fonctionnement des GLMMs et comment les implémenter dans `R`.

Dans le jeu de données `Arabidopsis`, l'effet de la disponibilité des nutriments et de l'herbivorie (**effets fixes**) sur la production de fruits (**variable de réponse**) d'*Arabidopsis thaliana* a été évalué en mesurant 625 plantes dans 9 populations différentes, chacune comprenant 2 à 3 génotypes différents (**effets aléatoires**).

Chargez les données `Arabidopsis` `banta_totalfruits.csv` dans `R`.

```{r, echo = TRUE, eval = TRUE}
dat.tf <- read.csv("data/banta_totalfruits.csv")

# Dans cet ensemble de données, les en-têtes de colonne sont définis comme suit:
# popu: facteur avec un niveau pour chaque population
# gen: facteur avec un niveau pour chaque génotype
# nutrient: facteur avec niveau bas (valeur = 1) ou haut (valeur = 8)
# amd: facteur précisant l'absence ou la présence d'herbivorie
# total.fruits: nombre entier indiquant le nombre de fruits par plante
```

# Choisir la distribution des erreurs

Nous devons maintenant choisir une distribution d'erreurs. Ce choix sera informé par la structure de nos données. 

La variable réponse constitue des données d'abondance, donc nous devons choisir une **distribution de Poisson** (i.e variance égale à la moyenne)

Jetons-y un coup d'oeil:

```{r, echo = TRUE, eval = TRUE}
#Avant d'aller plus loin, nous devons choisir une distribution d'erreurs. Ce choix sera guidé par la structure de nos données. 
# Notre variable de réponse est une donnée de comptage, ce qui suggère que nous avons besoin d'une distribution de Poisson (c'est-à-dire que la variance est égale à la moyenne).
hist(dat.tf$total.fruits, breaks = 50, col = 'blue', main = '',
     xlab = 'Total fruits', ylab = 'Count')
```

Cependant, comme nous le verrons, la variance de chaque groupe augmente beaucoup plus rapidement que prévu...

---

**Exploration de la variance **

Examinons de plus près la variance au sein de nos données.

Pour illustrer l'hétérogénéité de la variance, nous allons créer des *boxplots* du **log** du nombre total de fruits (**variable réponse**) par rapport aux différents facteurs environnementaux.

Créons d'abord de nouvelles variables qui représentent toutes les combinaisons de **nutriments** x **herbivorie** x **facteur aléatoire**.

```{r, echo = TRUE, eval = TRUE}
# Explorons la variance dans nos données
# Créez de nouvelles variables qui représentent chaque combinaison de variables.
dat.tf <- within(dat.tf,
{
  # genotype x nutrient x clipping
  gna <- interaction(gen,nutrient,amd)
  gna <- reorder(gna, total.fruits, mean)
  
  # population x nutrient x clipping
  pna <- interaction(popu,nutrient,amd)
  pna <- reorder(pna, total.fruits, mean)
})
```

Maintenant, visualisons:


```{r, echo = TRUE, eval = TRUE}
# Boxplot of total fruits vs genotype x nutrient x clipping interaction
ggplot(data = dat.tf, 
       aes(factor(x = gna), y = log(total.fruits + 1))) +
  geom_boxplot(colour = "skyblue2", 
               outlier.shape = 21,
               outlier.colour = "skyblue2") +
  ylab("log (Total fruits)\n") + # \n creates a space after the title
  xlab("\nGenotype x nutrient x clipping") + # space before the title
  theme_bw() + theme(axis.text.x = element_blank()) +
  stat_summary(fun = mean, geom = "point", colour = "red")
```


De même, la variance du total de fruits montre une grande hétérogénéité entre les populations (population x nutriments x herbivorie).

---

**Retour au choix de la distribution des erreurs** 

Comme nous venons de voir, il existe une importante hétérogénéité parmi la variance de chaque groupe, même lorsque la variable réponse est transformée (i.e. log).

Pour identifier la famille de distribution la plus appropriée, nous pouvons examiner un graphique de diagnostic de **la variance de chaque groupe par rapport à leurs moyennes**. Nous présentons un exemple ci-dessous pour le regroupement par génotype x nutriments x herbivore (clipping).

- 1. Si nous observons une relation linéaire entre la variance et la moyenne avec une pente = 1, une famille de Poisson serait appropriée,

- 2. Si nous observons une relation moyenne-variance linéaire avec une pente> 1 (c. $Var = \varphi\mu$ où $\varphi > 1$), la famille quasi-Poisson (tel que présenté ci-dessus) doit être utilisée,

- 3. Enfin, une relation quadratique entre la variance et la moyenne (c. $Var = \mu(1+\alpha) or \mu(1+\mu/k)$), est caractéristique des données surdispersées résultant d'une hétérogénéité sous-jacente entre les échantillons. Dans ce cas, la distribution binomiale négative (Poisson-gamma) serait plus appropriée.

```{r, echo = TRUE, eval = TRUE}
# Figure diagnostique de variances par groupe vs moyennes par groupe (genotype x nutrient x clipping grouping). 
# Code pour créer la figure: https://github.com/QCBSRworkshops/workshop07/blob/main/pres-fr/data/glmm_e.r

# On remarque beaucoup de variation entre les variances des échantillons dans les données transformées 
# Par exemple, entre génotypes:
grpVars <- tapply(dat.tf$total.fruits, dat.tf$gna, var)

grpMeans <- tapply(dat.tf$total.fruits,dat.tf$gna, mean)

# Quasi-Poisson
lm1 <- lm(grpVars~grpMeans-1) 
phi.fit <- coef(lm1)
# Le -1 specifie un modèle avec une ordonnée à l'origine de 0

# Binomiale négative
lm2 <- lm(grpVars ~ I(grpMeans^2) + offset(grpMeans)-1)
k.fit <- 1/coef(lm2)
# offset() spécifie qu'on veut ajouter un terme pour les moyennes de groupes avec un coefficient fixé à 1

# Non-parametric loess fit
Lfit <- loess(grpVars ~ grpMeans)

# La figure
plot(grpVars ~ grpMeans, xlab = "Group means", ylab = "Group variances" )
abline(a = 0, b = 1, lty = 2)
text(105,500, "Poisson")
curve(phi.fit*x, col = 2, add = TRUE)
# bquote() remplace des valeurs numériques dans des équations avec des symboles
text(110,3900,
     bquote(paste("QP: ", sigma^2==.(round(phi.fit,1))*mu)), col = 2)
curve(x*(1+x/k.fit), col = 4, add = TRUE)
text(104,7200, paste("NB: k = ", round(k.fit, 1), sep = ""), col = 4)
mvec <- 0:120
lines(mvec, predict(Lfit, mvec), col = 5)
text(118, 2000, "loess", col = 5)
```

La figure ci-dessus montre qu'un modèle linéaire quasi-Poisson sera peut-être mieux qu'une binomiale négative, mais il faudrait *continuer le processus de modélisation*.


# GLMM Poisson 

**Compte tenu de la relation moyenne-variance, nous avons besoin d'un modèle qui tient compte de la surdispersion.**

Pour comprendre pourquoi, commençons avec un modèle avec une distribution de Poisson.

Pour lancer un GLMM dans `R`, nous faisons appel à la fonction `glmer()` de la librairie `lme4`:

```{r, echo = TRUE, eval = TRUE}
# GLMM Poisson
# Nous avons besoin d'un modèle qui tient compte de la surdispersion.
# Commençons avec un modèle Poisson
mp1 <- glmer(total.fruits ~ nutrient*amd + rack + status +
             (1|popu)+
             (1|gen),
             data = dat.tf, family = "poisson")
```

**Effets aléatoires** : `(1|popu)` et `(1|gen)`. Nous faisons varier les ordonnées à l'origine pour les différentes populations (`popu`) et génotypes (`gen`).

---

**Vérification de la surdispersion**

On vérifie la surdispersion en utilisant la fonction `overdisp_fun()` (Bolker *et al*. 2011) qui divise la déviance des résidus (de Pearson) par leurs degrés de liberté.

La fonction teste si le **rapport est plus grand que 1**.

Effectuons ce test:

```{r}
# Téléchargez le code glmm_funs.R de la page wiki et exécutez la fonction dans R
source(file = "data/glmm_funs.R")
# Surdispersion?
overdisp_fun(mp1)
# Le rapport est significativement > 1
```

**Le rapport est significativement > 1**

Comme on s'y attendait, nous devons modéliser une **distribution différente** où la variance augmente plus rapidement que la moyenne.


# GLMM binomiale négative

Une option pour une distribution où la variance augmente plus rapidement avec la moyenne est la distribution **binomiale négative** (ou Poisson-gamma). Rappelons que la distribution binomiale négative répond à l'hypothèse selon laquelle la variance est proportionnelle au carré de la moyenne.

On peut modéliser cette distribution avec la fonction `glmer.nb()`:

```{r, echo = TRUE, eval = TRUE}
# GLMM binomial négatif utilisant la fonction glmer.nb()
mnb1 <- glmer.nb(total.fruits ~ nutrient*amd + rack + status +
                 (1|popu)+
                 (1|gen),
                 data = dat.tf,
                 control = glmerControl(optimizer = "bobyqa"))
# 'control' spécifie la façon dont nous optimisons les valeurs des paramètres
```

On teste encore une fois la surdispersion du modèle:

```{r, echo = TRUE, eval = TRUE}
# Surdispersion?
overdisp_fun(mnb1)
# Le rapport est beaucoup plus près de 1 mais la valeur de p < 0.05
```

**Le rapport est beaucoup plus près de 1 mais la valeur de p < 0.05**

# GLMM Poisson-lognormale

Une deuxième option pour une distribution où la variance augmente plus rapidement avec la moyenne est la distribution **Poisson-lognormale**.

On réalise cette distribution en ajoutant un **effet aléatoire de niveau d'observation** dans le modèle.

Voir Harrison (2014) pour plus de détails https://doi.org/10.7717/peerj.616.

Pour ce faire, nous créons tout d'abord une variable nommée `X`:

```{r, echo = TRUE, eval = TRUE}
# GLMM Poisson-lognormale

# Cette variable est déjà dans vos données "dat.tf", mais voici comment la créer.
dat.tf$X <- 1:nrow(dat.tf)
```

On traite la surdispersion en ajoutant l'effet `(1|X)` dans la formule:

```{r, echo = TRUE, eval = TRUE}
# Tenir compte de la surdispersion
mpl1 <- glmer(total.fruits ~ nutrient*amd + rack + status +
              (1|X) +
              (1|popu)+
              (1|gen),
data = dat.tf, family = "poisson",
control = glmerControl(optimizer = "bobyqa"))
```

On teste finalement pour la présence de surdispersion:

```{r, echo = TRUE, eval = TRUE}
# Surdispersion?
overdisp_fun(mpl1)
# Le rapport est maintenant conforme avec notre critère, soit < 1
```

**Le rapport est maintenant conforme avec notre critère, soit < 1!**

---

**Maintenant que nous avons choisi notre distribution d'erreur, visualisons les paramètres du modèle.**

Peut être obtenu en utilisant la fonction `coefplot2()`de la librairie `coefplot2`:

*Notez: Cette librairie n'est pas sur le CRAN! On utilise la librairie `remotes` pour l'installer à partir de GitHub.*

```{R install_coefplot2, eval = TRUE, results = hide}
# Visualisons les paramètres du modèle

# Cette librairie n'est pas sur le CRAN! On utilise la librairie `remotes` pour l'installer à partir de GitHub.
if (!require("coefplot2"))
  remotes::install_github("palday/coefplot2", subdir = "pkg")
library(coefplot2)
```


```{r, echo = TRUE, eval = TRUE}
# Effets aléatoires
coefplot2(mpl1, ptype = "vcov", intercept = TRUE, main = "Random effect variance")
```

```{r, echo = TRUE, eval = TRUE}
# Effets fixes
coefplot2(mpl1, intercept = TRUE, main = "Fixed effect coefficient")
```

*Notez: Les barres d'erreur sont visibles seulement pour les effets fixes, car `glmer()` ne modélise pas d'incertitude pour les effets aléatoires.*

---

**Visualisation des effets aléatoires**

Vous pouvez extraire les effets aléatoires en utilisant la fonction `ranef()` et les tracer en utilisant un `dotplot()` de la librairie `lattice`.

On constate une variation **inter-population**: 

- Les populations espagnoles (SP) ont des valeurs plus élevées que les populations suédoises (SW) et néerlandaises (NL)

On constate une faible variation **inter-génotype** :

- Les différences entre les génotypes semblent induites par le génotype 34

```{r, echo = TRUE, eval = TRUE}
# dotplot code
pp <- list(layout.widths = list(left.padding = 0, right.padding = 0),
           layout.heights = list(top.padding = 0, bottom.padding = 0))
r2 <- ranef(mpl1, condVar = TRUE)
d2 <- dotplot(r2, par.settings = pp)

grid.arrange(d2$gen, d2$popu, nrow = 1)
```


